{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be importing an admission related dataset with 400 tuples. Attributes: gre, gpa, rank & admit(admission: 1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\n",
    "X_data = data[['gre', 'gpa', 'rank']].values\n",
    "Y_data = data[['admit']].values\n",
    "X1_data = data[['gre']].values\n",
    "X2_data = data[['gpa']].values\n",
    "X3_data = data[['rank']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tuples:  400\n"
     ]
    }
   ],
   "source": [
    "n_samples = Y_data.shape[0]\n",
    "print(\"Number of tuples: \", n_samples)\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 50\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "X1 = tf.placeholder(\"float\")\n",
    "X2 = tf.placeholder(\"float\")\n",
    "X3 = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Set model weights\n",
    "W1 = tf.Variable(rng.randn(), name=\"weight1\")\n",
    "W2 = tf.Variable(rng.randn(), name=\"weight2\")\n",
    "W3 = tf.Variable(rng.randn(), name=\"weight3\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "#Our Linear Model\n",
    "hypothesis = tf.add(tf.add(tf.multiply(X1, W1), tf.add(tf.multiply(X2, W2), tf.multiply(X3, W3))), b)\n",
    "\n",
    "# Cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(hypothesis-Y, 2))/(2*n_samples)\n",
    "\n",
    "#Gradient descent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 cost= 0.000015701 W1= 0.0034705282 W2= -0.69725955 W3= -0.04900788 b= 0.6649732\n",
      "Epoch: 0010 cost= 0.002250475 W1= -0.001579304 W2= -0.24156581 W3= -0.102044664 b= 0.85162514\n",
      "Epoch: 0015 cost= 0.000080254 W1= -0.00030835113 W2= -0.07990155 W3= -0.18019259 b= 0.7830223\n",
      "Epoch: 0020 cost= 0.000039695 W1= -0.00029018451 W2= -0.022743983 W3= -0.19990839 b= 0.6841078\n",
      "Epoch: 0025 cost= 0.000026464 W1= -0.00027164654 W2= 0.0068798475 W3= -0.19782294 b= 0.5841922\n",
      "Epoch: 0030 cost= 0.000019146 W1= -0.0002544017 W2= 0.030159544 W3= -0.19472803 b= 0.49574333\n",
      "Epoch: 0035 cost= 0.000014210 W1= -0.00024122745 W2= 0.050078418 W3= -0.19179401 b= 0.4186921\n",
      "Epoch: 0040 cost= 0.000010753 W1= -0.0002314928 W2= 0.06733115 W3= -0.18913558 b= 0.35163596\n",
      "Epoch: 0045 cost= 0.000008295 W1= -0.0002244024 W2= 0.0823136 W3= -0.18675268 b= 0.2932402\n",
      "Epoch: 0050 cost= 0.000006523 W1= -0.00021930109 W2= 0.09534109 W3= -0.18462905 b= 0.2423512\n",
      "Optimization Finished!\n",
      "Training cost= 6.5232225e-06 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x1, x2, x3, y) in zip(X1_data, X2_data, X3_data, Y_data):\n",
    "            sess.run(optimizer, feed_dict={X1: x1, X2: x2, X3: x3, Y: y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X1: x1, X2: x2, X3: x3, Y: y})\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W1=\", sess.run(W1), \"W2=\", sess.run(W2),\"W3=\", sess.run(W3),\"b=\", sess.run(b))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X1: x1, X2: x2, X3: x3, Y: y})\n",
    "    print (\"Training cost=\", training_cost,'\\n')\n",
    "    \n",
    "    #Graph to be added\n",
    "    #Evaluation on test (after train_test_split) to be added\n",
    "    #Seems to overfit because the Cost is too damn low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
